模糊匹配，即对单词进行划分，再枚举每个部分用什么同音拼写代替，
如果能够匹配成功，则将模糊匹配到的单词推到首位。
辅音+元音+（r/l/n/m/g）
注意辅音双写的特例！
注意不发音的特例! (gh)
semelur->s\e\m\e\l\ur->s\i\m\i\l\ar->similar
余下的推荐列表依然根据前缀匹配显示。


每次查找一个单词的复杂度是O(logN)（用map存储词库）
假设有k个音节，枚举j个音节，有O(C(k,j))种，每种替换假设的分支数假设分别为
a1,a2,...aj,则需要枚举O(a1a2..aj)次。

增加一个按钮，可以随时 开启/关闭 智能匹配功能。

根据github上面共同编辑的音节表，
任意两个字母组合，如果在同一个发音行出现过，互相建立关联。
具体存储方式是开一个map，里面每个字母组合都建立一个list，表示和它有关联的字母组合。
每次代码启动的时候自动解析字母组合表，所以只要更新字母组合表，重启代码就可以看到更新效果。
划分单词的时候每次都向后找到最长的能够在表中找到的字母组合。
会不会一个字母组合是另一个的前缀？有可能，但是这种情况考虑更长的组合是更合理的，
因为这种现象几乎只会出现在元音中，而单词中元音一定是连续的段，应当考虑局部最长的元音组合。
例如 tour，our肯定是连续的一个组合，而不是只将o这个前缀作为一个划分。

数据集：
1.组合表
2.辅音/元音 标定表
3.测试数据

优化：
1.用户打错音节的数量越多，这种情况出现的概率越小，所以按照替换数量从小到大的
顺序替换，可以更大概率提高用户获得目标信息的速度。
2.并不是同一个发音的组合都可以互相混淆，例如good中的oo，
虽然和ew、ue发音相同，但是几乎不可能有人输入后者，因此这类枚举往往
是无效的，我们可以根据这种判断将oo和ew、ue分裂成更细化的小组，减小每个组的容量，从而降低算法复杂度。 （开音节和闭音节是一个很大的区分点，另外如果某两个很短的字母组合只会在极其特定的情况下混淆，那么就单独建立那种联系，而不应该无脑建立两个短组合的联系，例如s和c，会在sion和cion中混淆，那么就单独建立sion和cion的组，而不是建立s和c的联系。）
3.单词最后的l直接不管。
4.优先替换元音部分，最后替换辅音部分。
5.结尾出现开音节形式的组合，直接分解，不需要继续拆分。
6.强力优化：对词库建立字典树，在替换单词的过程中，时刻判断是否满足当前单词是不是字典树前缀，如果不是，直接退出当前搜索分支。

7.新功能：给每一个特定组的词之间设置一个改变距离，最终得到改变的总距离，根据距离从小到大排序，越接近用户输入词的词排在前面。


细节：
1.开音节问题。
单词可以尝试在结尾+e/-e
比如单词结尾如果是辅音组合+e的形式，可能e不发音(pipe、site，bike)。
2.gh不发音，可能需要添加。 例如fou(gh)t,用户输入的是fault，程序需要能够将aul替换成ou，再添加gh，得到fought.
3.有些元音组合只在特定情况下可以被视为一个组合。例如ir,只有在形如b(ir)d这种后面是辅音的情况下才会在一起，
这种情况只需要在组合表里面手动加入ird，将其强行变成一个组合，优先识别就可以了。
4.浊化！
5.有趣的现象：到了后期，甚至反过来要去掉一些特殊组合，给程序减负，以增加程序的自由度。

答辩的要点：
1.要有数据，实验数据。
测试技巧：
建立一个错误单词输入数据集，每次跑程序看看要多少秒，正确率如何，实现量化评估。

2.要有算法，体现出我们的思考。
3.演示部分，有创意，有艺术。

semeler
this
these
ah ar
u a
